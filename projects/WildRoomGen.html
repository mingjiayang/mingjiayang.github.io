<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" slick-uniqueid="3">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="keywords" content="Ming-Jia Yang, Beihang University, 杨明佳, 北京航空航天大学, 3D Scene Generation">
<meta name="description" content="WildRoomGen: 3D room generation from in-the-wild indoor image collections">
<link rel="stylesheet" href="../homepage_files/style/jemdoc.css" type="text/css">
<style type="text/css">
</style>
<title>WildRoomGen</title>
</head>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.3.0/model-viewer.min.js"></script>


<body>
<div id="layout-content" style="margin-top:25px">

<!-- teaser -->
<table border="0" width="100%"> <tbody>
<tr>
  <td valign="top" align="center">
    <b><font size="5" face="Times New Roman" >WildRoomGen: 3D Room Generation from In-the-Wild Indoor Image Collections</font></b>
    <br><br>
  </td>
</tr>

<tr>
  <td valign="top" align="center">
    <font size="4" face="Times New Roman" > <a href="https://mingjiayang.github.io/"  target="_blank">Ming-Jia Yang</a> </font> <font size="2"><sup>1</sup></font> &emsp;&emsp;
    <font size="4" face="Times New Roman" > <a href="https://xueyuhanlang.github.io/"  target="_blank">Yang Liu</a> </font> <font size="2"><sup>2</sup></font> &emsp;&emsp;
    <font size="4" face="Times New Roman" > <a href="https://scse.buaa.edu.cn/info/1079/7270.htm"  target="_blank">Bin Zhou</a> </font> <font size="2"><sup>1</sup></font> &emsp;&emsp;
    <br> 
    <font size="2"> <sup>1</sup></font> <font size="3" face="Times New Roman" > Beihang University </font>
    &emsp;&emsp;
    <font size="2"> <sup>2</sup></font> <font size="3" face="Times New Roman" > Microsoft Research Asia </font>
    &emsp;&emsp;
    <br>
    <br>
  </td>
</tr>

<tr>
  <td valign="top" align="center">
      <img width="100%" src="./WildRoomGen/teaser.png">
  </td>
</tr>
</tbody> </table>


<!-- Abstract -->
<table border="0" width="100%"> <tbody>
<h2>Abstract</h2>
<tr>
  <p style="text-align:justify;">
  <font face="Times New Roman" >
    3D indoor scene generation is challenging due to the lack of high-quality and diverse 3D datasets. Recent generative approaches using in-the-wild image collections offer promising solutions, but issues with scene quality, diversity, and multi-view consistency persist. In this paper, we introduce WildRoomGen, an efficient image-conditioned 3D room generation framework designed to overcome these limitations. WildRoomGen comprises two key components: (1) RoomGen, a GAN-based single-view conditioned 3D room generator that learns from large-scale, single-view room images to generate diverse NeRF-based 3D rooms. RoomGen significantly improves generation quality and diversity through enhanced camera estimation, perspective projection-based image feature embedding, and the utilization of pretrained image feature and pseudo-depth priors. (2) RoomRecon, a feedforward NeRF reconstruction network that addresses 3D inconsistency issues of RoomGen and prior methods due to the use of image super-resolution for image enhancement,  while being trained solely on RoomGen's generated results without the need for 3D room data. We extensively evaluate the quality and diversity of the 3D rooms generated by WildRoomGen, highlighting its effectiveness and efficiency. Furthermore, we demonstrate the generality of our approach and its scalability to data sizes.
  </font>
  </p>
</tr>

<table border="0" width="100%"> <tbody>
  <h2>Method</h2>
    <!-- <h3>WildRoomGen framework</h3> -->
    <tr>
      <p style="text-align:justify;">
      <font face="Times New Roman" >
        The framework overview of WildRoomGen. Given a single-view room image as input, RoomGen extracts its DiNO features and projects them into triplanes via perspective projection. After combining the noise and room-size features, the triplane features are decoded as a NeRF, whose rendered color images and depth images are used for training the generator, discriminator, and image camera pose estimator. RoomRecon takes multi-view super-resolution rendering images of WildRoomGen as inputs to reconstruct a triplane-based NeRF that improves rendering fidelity while ensuring multi-view consistency. RoomGen uses in-the-wild images for training only and utilizes in-shelf pretrained image encoders and pseudo-depth priors. RoomRecon is trained solely on RoomGen's results.
      </font>
      </p>
      <td align="center"> <img width="90%" src="./WildRoomGen/overview.png"></td>
    </tr>



<!-- Results -->
<table border="0" width="100%"> <tbody>
  <h2>Results</h2>
  <tr>
    <center>
      <video width="80%" height="80%" controls>
        <source src="./WildRoomGen/results-video.mp4" type="video/mp4">
      </video>
    </center>
    
    <p style="text-align:center;">
    <font face="Times New Roman" >
      Various 3D scenes created by WildRoomGen, given AI-generated image as condition.
    </font>
    </p>
  </tr>
<tr>

<!-- Results -->
<table border="0" width="100%"> <tbody>
  <h2>Comparision</h2>
  <tr>
    <center>
      <video width="80%" height="80%" controls>
        <source src="./WildRoomGen/comparison-video.mp4" type="video/mp4">
      </video>
    </center>

    <p style="text-align:center;">
    <font face="Times New Roman" >
      Qualitative comparisons with Text2Room and ZeroNVS.
    </font>
    </p>
  </tr>
<tr>


<!-- <table border="0" width="100%"> <tbody>
  <h2>Links</h2>
<td align="center" width="25%">
  <img src="./WildRoomGen/teaser.png" width="200">
</td>
<td>
  <strong>Paper</strong> [<a href="https://arxiv.org/abs/2504.02337" target="_blank">arXiv</a>] <br><br>
  </p>
</td>
</tr>
</tbody> </table> -->


</div>
</body>
</html>